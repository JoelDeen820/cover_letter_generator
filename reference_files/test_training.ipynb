{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# read pickle file\n",
    "pickle_in = open(\"plots_text.pickle\",\"rb\")\n",
    "movie_plots = pickle.load(pickle_in)\n",
    "\n",
    "# count of movie plot summaries\n",
    "print(len(movie_plots))\n",
    "\n",
    "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i).lower() for i in movie_plots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_seq(text, seq_len = 5):\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    # if the number of tokens in 'text' is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "      for i in range(seq_len, len(text.split())):\n",
    "        # select sequence of tokens\n",
    "        seq = text.split()[i-seq_len:i+1]\n",
    "        # add to the list\n",
    "        sequences.append(\" \".join(seq))\n",
    "\n",
    "      return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "\n",
    "      return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152644"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [create_seq(i) for i in movie_plots]\n",
    "\n",
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])\n",
    "\n",
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591, 'ensue')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(movie_plots).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "\n",
    "token2int[\"the\"], int2token[14271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "\n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      x = arr_x[prv:n,:]\n",
    "      y = arr_y[prv:n,:]\n",
    "      prv = n\n",
    "      yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResumeNLP(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size : int, n_hidden=256, n_layers=4, dropout_prob=0.3, lr=0.003):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = dropout_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.intermediate_layer_size = 200\n",
    "        self.emb_layer = nn.Embedding(vocab_size, self.intermediate_layer_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.intermediate_layer_size, self.n_hidden, self.n_layers,\n",
    "                                dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc = nn.Linear(self.n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.emb_layer(x)\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        out = self.dropout(lstm_output)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else :\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeNLP(\n",
      "  (emb_layer): Embedding(16592, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=32, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(int2token)\n",
    "# instantiate the model\n",
    "net = ResumeNLP(vocab_size)\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "\n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "\n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 3.82 GiB total capacity; 2.99 GiB already allocated; 305.31 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train(net, batch_size \u001B[39m=\u001B[39;49m \u001B[39m1024\u001B[39;49m, epochs\u001B[39m=\u001B[39;49m\u001B[39m20\u001B[39;49m, print_every\u001B[39m=\u001B[39;49m\u001B[39m256\u001B[39;49m)\n",
      "Cell \u001B[0;32mIn [21], line 43\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(net, epochs, batch_size, lr, clip, print_every)\u001B[0m\n\u001B[1;32m     40\u001B[0m loss \u001B[39m=\u001B[39m criterion(output, targets\u001B[39m.\u001B[39mview(\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m))\n\u001B[1;32m     42\u001B[0m \u001B[39m# back-propagate error\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m loss\u001B[39m.\u001B[39;49mbackward()\n\u001B[1;32m     45\u001B[0m \u001B[39m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m nn\u001B[39m.\u001B[39mutils\u001B[39m.\u001B[39mclip_grad_norm_(net\u001B[39m.\u001B[39mparameters(), clip)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[39mif\u001B[39;00m has_torch_function_unary(\u001B[39mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[39mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[39m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[39mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[39m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m torch\u001B[39m.\u001B[39;49mautograd\u001B[39m.\u001B[39;49mbackward(\n\u001B[1;32m    488\u001B[0m     \u001B[39mself\u001B[39;49m, gradient, retain_graph, create_graph, inputs\u001B[39m=\u001B[39;49minputs\n\u001B[1;32m    489\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[39m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[39m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[39m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[39m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m Variable\u001B[39m.\u001B[39;49m_execution_engine\u001B[39m.\u001B[39;49mrun_backward(  \u001B[39m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001B[1;32m    199\u001B[0m     allow_unreachable\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m, accumulate_grad\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 3.82 GiB total capacity; 2.99 GiB already allocated; 305.31 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(net, batch_size = 1024, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "\n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "\n",
    "  # push to GPU\n",
    "  inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "  top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "\n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "      token, h = predict(net, t, h)\n",
    "\n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is the first thing she has not been waiting by his family and that the two'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the film is a vampire of divine sports in a small village with a neighboring life'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"one of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
